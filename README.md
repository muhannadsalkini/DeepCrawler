# DeepCrawler Project Documentation

## Overview

DeepCrawler is a production-ready web crawler API built with Node.js and TypeScript. It supports single URL scraping, batch processing, and intelligent crawling with configurable strategies and safety limits.

## ğŸ¯ Project Goals

- **Single URL Scraping**: Extract content, title, text, and links from any URL
- **Batch Scraping**: Process multiple URLs concurrently with aggregated results
- **Intelligent Crawling**: Two crawl strategies:
  - **Domain-only**: Stay within the same domain
  - **Full-link**: Follow all discovered links
- **Progress Tracking**: Monitor crawl jobs in real-time via job IDs
- **Scalable Design**: Architecture ready for Redis and worker pools

## ğŸ§± Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Runtime | Node.js 20+ | Modern JavaScript runtime |
| Language | TypeScript | Type safety and better DX |
| API Framework | Fastify | High-performance HTTP server |
| HTTP Client | Undici | Fast, modern HTTP client |
| HTML Parser | Cheerio | jQuery-like HTML parsing |
| Validation | Zod | Schema validation |
| Rate Limiting | Bottleneck | Request rate control |
| Browser (Optional) | Playwright | Fallback for JS-heavy sites |

## ğŸ— Architecture

DeepCrawler follows a modular, layered architecture:

```mermaid
graph TB
    subgraph "API Layer"
        A[Fastify Server]
        B[Route Handlers]
    end
    
    subgraph "Job Management"
        C[Job Manager]
        D[Progress Tracker]
    end
    
    subgraph "Crawler Engine"
        E[Crawl Engine]
        F[BFS Queue]
        G[URL Normalizer]
        H[Rules Engine]
    end
    
    subgraph "Content Processing"
        I[Fetcher - Undici]
        J[Parser - Cheerio]
        K[Link Extractor]
    end
    
    A --> B
    B --> C
    C --> E
    E --> F
    E --> G
    E --> H
    E --> I
    I --> J
    J --> K
    K --> F
    E --> D
    D --> C
```

### Key Components

#### API Layer
- **Fastify Server**: Handles HTTP requests and responses
- **Route Handlers**: Clean separation of routes and business logic

#### Job Management
- **Job Manager**: Creates, tracks, and manages crawl jobs
- **Progress Tracker**: Real-time metrics and status updates

#### Crawler Engine
- **Crawl Engine**: Orchestrates the entire crawl process
- **BFS Queue**: Breadth-first search implementation
- **URL Normalizer**: Ensures consistent URL format
- **Rules Engine**: Enforces limits and filters

#### Content Processing
- **Fetcher**: Downloads HTML content using Undici
- **Parser**: Extracts structured data using Cheerio
- **Link Extractor**: Discovers and normalizes links

## ğŸ“ Project Structure

```
DeepCrawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ crawl.ts          # Crawl endpoints
â”‚   â”‚       â””â”€â”€ status.ts         # Status endpoints
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ engine.ts             # Main crawl orchestrator
â”‚   â”‚   â”œâ”€â”€ queue.ts              # BFS queue implementation
â”‚   â”‚   â”œâ”€â”€ rules.ts              # Crawl rules and filters
â”‚   â”‚   â”œâ”€â”€ metrics.ts            # Progress tracking
â”‚   â”‚   â”œâ”€â”€ fetcher.ts            # HTTP fetching
â”‚   â”‚   â”œâ”€â”€ parser.ts             # HTML parsing
â”‚   â”‚   â”œâ”€â”€ extractor.ts          # Link extraction
â”‚   â”‚   â””â”€â”€ normalizer.ts         # URL normalization
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ job-manager.ts        # Job lifecycle management
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.ts             # Logging utilities
â”‚   â”‚   â””â”€â”€ timer.ts              # Timing utilities
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ crawl.ts              # TypeScript types
â”‚   â””â”€â”€ server.ts                 # Entry point
â”œâ”€â”€ tests/
â”œâ”€â”€ .env.example
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ” Crawling Strategies

### Domain-Only Crawling
Stays within the same domain as the starting URL.

**Use cases:**
- Site mapping
- Content auditing
- SEO analysis

**Example:**
```
Start: https://example.com/page1
âœ“ https://example.com/page2
âœ“ https://example.com/blog/post
âœ— https://other-site.com/page
```

### Full-Link Crawling
Follows all discovered links regardless of domain.

**Use cases:**
- Web archiving
- Research
- Discovery

**Example:**
```
Start: https://example.com/page1
âœ“ https://example.com/page2
âœ“ https://other-site.com/page
âœ“ https://third-site.com/resource
```

## ğŸš¦ Safety & Politeness

DeepCrawler implements several safety mechanisms:

- **Request Timeout**: Prevents hanging on slow sites
- **Max Response Size**: Limits memory usage
- **Rate Limiting**: Prevents overwhelming servers
- **User-Agent**: Identifies the crawler
- **Concurrency Limits**: Controls parallel requests
- **Depth Limits**: Prevents infinite crawls
- **Page Limits**: Caps total pages scraped
- **robots.txt**: Optional respect for site preferences
- **IP Filtering**: Blocks private/localhost crawling

## ğŸ“Š Metrics & Tracking

Each crawl job tracks:

| Metric | Description |
|--------|-------------|
| `pagesScraped` | Total pages successfully processed |
| `linksDiscovered` | Unique links found |
| `errors` | Count and details of failures |
| `depth` | Current and maximum depth reached |
| `duration` | Total crawl time |
| `status` | Current job state |

## ğŸ”® Future Scalability

The architecture is designed for future enhancements:

### Phase 1: Redis Integration
- Move queue to Redis
- Distributed visited set
- Persistent job storage

### Phase 2: Worker Pools
- Separate API and workers
- Horizontal scaling
- Load balancing

### Phase 3: Advanced Features
- JavaScript rendering (Playwright)
- Content extraction rules
- Data export formats
- Webhook notifications

## ğŸ“ Development Phases

The project is built in 8 phases:

1. **Project Setup** - Initialize tooling and health endpoint
2. **Single URL Scraper** - Core scraping functionality
3. **Batch Scraping** - Concurrent processing
4. **Crawl Engine** - BFS algorithm and strategies
5. **Job System** - Progress tracking and status
6. **Safety & Politeness** - Rate limiting and guards
7. **Redis Readiness** - Interface abstractions
8. **Documentation** - Comprehensive guides and examples

## âœ… Success Criteria

The project is complete when:

- âœ… Crawl jobs can be started via API
- âœ… Progress can be queried in real-time
- âœ… Domain and limit restrictions are enforced
- âœ… Results include discovered links and metrics
- âœ… System is ready for Redis and worker migration
- âœ… Documentation is comprehensive and clear

## ğŸ”— Quick Links

- [Architecture Guide](./ARCHITECTURE.md)
- [API Documentation](./API.md)
- [Development Guide](./DEVELOPMENT.md)
- [Task Breakdown](./task.md)

---

**Next Steps**: See [task.md](./task.md) for the complete development checklist.
